{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing for NLP\n",
    "\n",
    "When working with text data there is some preprocessing that needs to be done before doing any modeling. The aim of this preprocessing is to remove as much noise as possible, while keeping as much information as possible. \n",
    "\n",
    "In the previous section, we observed two things already:\n",
    "\n",
    "1. There are lots of `&amp`, `&quot` and symbols like that in the tweets. These symbols are html markers that don't provide much information or meaning, and could be removed for the purpose of sentiment analysis.\n",
    "2. By far, the most frequest words, are words of the kind: `I, the, a, you, ...`. These words don't carry any sentiment meaning and flood the tweets. If we keep them in there, they will make it much harder for any algorythm to learn. Actually, these words are called [stop words](https://en.wikipedia.org/wiki/Stop_word) and it is very common to remove them for any NLP task.\n",
    "\n",
    "We will do more processing steps, but let's start by these two. \n",
    "\n",
    "In this directory you will find a file called `twitter.py`, this file contains the definition of a class `TextProcessor` that we will be filling out. The first function that we will implement, will remove all these html markers from the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't modify this cell. This will make any changes in the twitter.py file to be immediately loaded in this notebook\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from twitter import TextProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep all your other imports in this cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the train_data.csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Print the first 50 rows' text, there are many interesting examples to see\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides what we already observed, there are other things that catch our eye. For example:\n",
    "\n",
    "* Mentions (i.e @106andpark) are meaningless. Can be removed\n",
    "* Punctuation signs (comma, dot, etc.) don't carry much information. While exclamation or question marks could carry some meaning, we can safely remove them, since the sentiment weight is carried in meaningful words. \n",
    "* In some tweets, there are emoticons like :) or (:. We could actually preserve them, since those _do_ mean something\n",
    "\n",
    "**NOTE:** Some of the following questions make use of Python [re](https://docs.python.org/3/library/re.html) module for regular expressions. If you have not worked with regular expressions before, these questions may be a bit tricky. Don't hesitate to ask for help in the classroom :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Modify the function remove_mentions in the TextProcessor class so that it removes mentions from a tweet. Run this cell\n",
    "# when you are done to check your solution. The cell should run without errors\n",
    "\n",
    "tp = TextProcessor()\n",
    "text = '@you this is @amention in a tweet'\n",
    "res = tp.remove_mentions(text)\n",
    "\n",
    "# Use the .strip() method to remove extra spaces created when removing mentions at the beginning or the end\n",
    "assert res == 'this is  in a tweet', print(f'Your result: {res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before removing punctuation signs, let's unescape the HTML characters from the tweets. These are the `&amp` or `&quot` that we have seen before, and unescaping them means to transform them to their character representations. In these cases `&` and `\"`. Don't worry, [this](https://docs.python.org/3/library/html.html) python module can do the work for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Modify the function unescape_html in the TextProcessor class so that it removes html markups like &amp or &quot. \n",
    "# Run this cell when you are done to check your solution. The cell should run without errors\n",
    "\n",
    "tp = TextProcessor()\n",
    "text = '&amp this contains html &quot;markup&quot'\n",
    "res = tp.unescape_html(text)\n",
    "\n",
    "assert res == '& this contains html \"markup\"', print(f'Your result: {res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we don't have mentions or escaped HTML characters, we can remove all other punctuation signs. But let's also try to keep the emojis as we think they carry sentiment information, right? :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Modify the remove_punctuation function in the TextProcessor class so that it removes any punctuation sign. If the parameter keep_emoticons=True\n",
    "# save the emojis and append them at the end of the resulting string\n",
    "\n",
    "tp = TextProcessor()\n",
    "text = 'This tweet is great! :) right?'\n",
    "res = tp.remove_punctuation(text)\n",
    "\n",
    "assert res == 'This tweet is great right :)', print(f'Your result: {res}')\n",
    "\n",
    "res = tp.remove_punctuation(text, keep_emoticons=False)\n",
    "\n",
    "assert res == 'This tweet is great right ', print(f'Your result: {res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! What else can we do to reduce noise? We could, for example, lowercase all the words. This way the same word capitalise or now, will be counted only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Modify the function lower in the TextProcessor class so that it transforms the tweet to lowercase. Run this cell when \n",
    "# you are done to check your solution. The cell should run without errors\n",
    "tp = TextProcessor()\n",
    "text = 'This is UpperCased'\n",
    "res = tp.lower(text)\n",
    "\n",
    "assert res == \"this is uppercased\", print(f'Your result: {res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nicely done! Let's move on to removing stop words. Of course, the list of stop words is very big, and it has already been sorted out for us. Python has a great library for NLP called [nltk](https://www.nltk.org/). If you haven't yet, install this library with `pip install nltk`. The class `TextProcessor` already initialises a list of stopwords for you to use. They are saved as a class attribute called `stop_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Using the list of stop words provided by nltk, modify the function remove_stopwords in the TextProcessor class\n",
    "# so that it removes any stop word from the given text. Run this cell when you are done to check your solution. \n",
    "# The cell should run without errors\n",
    "tp = TextProcessor()\n",
    "text = 'I am really loving this course. I think I am learning so much already!'\n",
    "res = tp.remove_stopwords(text)\n",
    "\n",
    "assert res == \"I really loving course. I think I learning much already!\", print(f'Your result: {res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're almost doe with the preprocessing! One last thing that is commonly done is to reduce the words to their root. This is called stemming, and the purpose is to reduce the vocabulary by only using the root termination of a word. For example `loving` and `love` would both become `love`. \n",
    "\n",
    "Again, `nltk` provides us with such a stemmer, called [Porter Steamer](https://www.nltk.org/api/nltk.stem.html#module-nltk.stem.porter). Use this module to complete the following question:\n",
    "\n",
    "**NOTE:** There are more stemming algorithms provided by NLTK, feel free to experiment with them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Modify the function stem in the TextProcessor class to convert all the words in the tweet to their root. Run \n",
    "# this cell when you are done to check your solution. The cell should run without errors\n",
    "\n",
    "tp = TextProcessor()\n",
    "text = 'I am really loving this course. I think I am learning so much already!'\n",
    "res = tp.stem(text)\n",
    "\n",
    "assert res == \"I am realli love thi course. I think I am learn so much already!\", print(f'Your result: {res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know some words look strange and become not even a word (i.e thi), but using only the root of the words reduces the\n",
    "vocabulary drastically, which makes the learning process easier for any algorithm. \n",
    "\n",
    "We are done! If you have successfully completed the previous tasks, the following cell should return a dataframe with all the tweets pre_processed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = TextProcessor()\n",
    "df = tp.preprocess(df)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this transformed dataframe, let's ask some of the questions we asked in the previous part of the project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Before processing, the number of unique words was 148857. How many are there now?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, stemming and removing trash has drastically reduced the vocabulary to work with. That's great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. What are now the most common 50 words? What are their frequency distribution?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
